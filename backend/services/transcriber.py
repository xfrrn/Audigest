import importlib.util
import os
from pathlib import Path
from typing import Dict, List, Literal, Optional

import requests
from loguru import logger

HAS_LOCAL_DEPS = importlib.util.find_spec("whisperx") is not None


# è¡¥ä¸å‡½æ•°
def _apply_torch_monkey_patch():
    import torch

    if getattr(torch, "_audigest_patched", False):
        return
    logger.debug("ğŸ”§ [Local] åº”ç”¨ PyTorch å…¼å®¹æ€§è¡¥ä¸...")
    _original_torch_load = torch.load

    def _safe_torch_load(*args, **kwargs):
        kwargs["weights_only"] = False
        return _original_torch_load(*args, **kwargs)

    torch.load = _safe_torch_load
    setattr(torch, "_audigest_patched", True)


class TranscriptionError(Exception):
    pass


class AudioTranscriber:
    def __init__(
        self,
        mode: Literal["local", "cloud"] = "local",
        api_key: Optional[str] = None,
        hf_token: Optional[str] = None,
        device: str = "cuda",
        output_dir: str = "data/processing",
    ):
        """
        åˆå§‹åŒ–è½¬å½•å™¨
        :param mode: 'local' (WhisperX) æˆ– 'cloud' (Deepgram)
        :param api_key: äº‘ç«¯æ¨¡å¼çš„ API Key (Deepgram Key)
        :param hf_token: æœ¬åœ°æ¨¡å¼ Diarization å¿…é¡»çš„ HuggingFace Token
        """
        self.mode = mode
        self.api_key = api_key
        self.hf_token = hf_token
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"[Transcriber] åˆå§‹åŒ–å®Œæˆ | æ¨¡å¼: {self.mode} | è®¾å¤‡: {self.device}")

    def transcribe(self, audio_path: str, language: str = "auto") -> List[Dict]:
        """
        ä¸»å…¥å£ï¼šå°†éŸ³é¢‘è½¬æ¢ä¸ºå¸¦æœ‰è¯´è¯äººçš„æ–‡æœ¬ç‰‡æ®µ
        :param audio_path: æœ¬åœ°éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„
        :param language: è¯­è¨€ä»£ç  ('zh', 'en', 'auto')
        :return: ç»“æ„åŒ–åˆ—è¡¨ [{'start': 0.5, 'end': 2.0, 'text': '...', 'speaker': 'SPEAKER_01'}]
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

        logger.info(f"[Transcriber] å¼€å§‹å¤„ç†: {audio_path} (Lang: {language})")

        try:
            if self.mode == "local":
                return self._transcribe_local_whisperx(audio_path)
            elif self.mode == "cloud":
                return self._transcribe_cloud_deepgram(audio_path, language=language)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {self.mode}")
        except Exception as e:
            logger.exception("[Transcriber] è½¬å½•å¤±è´¥")
            raise TranscriptionError(str(e)) from e

    def _transcribe_local_whisperx(self, audio_path: str) -> List[Dict]:
        if not HAS_LOCAL_DEPS:
            raise ImportError("æœªå®‰è£… whisperx æˆ– torchï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å¼ã€‚è¯·è¿è¡Œ uv add git+https://github.com/m-bain/whisperX.git")
        if not self.hf_token:
            logger.warning("âš ï¸ æœªæä¾› HuggingFace Tokenï¼Œæ— æ³•è¿›è¡Œè¯´è¯äººåˆ†ç¦» (Diarization)ï¼Œä»…èƒ½è½¬å½•æ–‡å­—ã€‚")
        _apply_torch_monkey_patch()
        import whisperx
        from whisperx.diarize import DiarizationPipeline

        # 1. åŠ è½½æ¨¡å‹
        model_name = "medium"
        compute_type = "int8" if self.device == "cuda" else "int8"
        logger.info(f"â³ [Local] æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ ({model_name}, {compute_type})...")
        model = whisperx.load_model(model_name, self.device, compute_type=compute_type)

        # 2. è½¬å½•
        logger.info("[Local] æ­£åœ¨è½¬å½•æ–‡æœ¬...")
        result = model.transcribe(audio_path, batch_size=4)

        # 3. å¯¹é½
        logger.info("[Local] æ­£åœ¨å¯¹é½æ—¶é—´è½´...")
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=self.device)
        result = whisperx.align(result["segments"], model_a, metadata, audio_path, self.device, return_char_alignments=False)

        # 4. è¯´è¯äººåˆ†ç¦»
        if self.hf_token:
            logger.info("[Local] æ­£åœ¨è¯†åˆ«è¯´è¯äºº (Diarization)...")
            diarize_model = DiarizationPipeline(use_auth_token=self.hf_token, device=self.device)
            diarize_segments = diarize_model(audio_path)

            result = whisperx.assign_word_speakers(diarize_segments, result)

        # 5. æ ¼å¼åŒ–è¾“å‡º
        final_segments = []
        for segment in result["segments"]:
            final_segments.append(
                {
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "speaker": segment.get("speaker", "Unknown"),
                }
            )

        logger.success(f"âœ… [Local] è½¬å½•å®Œæˆï¼Œå…± {len(final_segments)} æ¡ç‰‡æ®µ")
        return final_segments

    def _transcribe_cloud_deepgram(self, audio_path: str, language: str = "auto") -> List[Dict]:
        if not self.api_key:
            raise ValueError("ä½¿ç”¨ Deepgram æ¨¡å¼å¿…é¡»æä¾› api_key")

        url = "https://api.deepgram.com/v1/listen"

        # 1. å‡†å¤‡å‚æ•°
        params = {
            "model": "nova-2",
            "smart_format": "true",  # è‡ªåŠ¨æ ‡ç‚¹ã€å¤§å°å†™
            "diarize": "true",  # å¼€å¯è¯´è¯äººåˆ†ç¦»
            "punctuate": "true",
            "utterances": "true",
        }

        # è¯­è¨€è®¾ç½®
        if language and language != "auto":
            params["language"] = language
        else:
            params["detect_language"] = "true"

        headers = {
            "Authorization": f"Token {self.api_key}",
            "Content-Type": "audio/*",
        }

        logger.info(f"[Deepgram] å¼€å§‹ä¸Šä¼ å¹¶è½¬å½• (è¯­è¨€: {language})...")

        # 2. å‘é€è¯·æ±‚
        try:
            with open(audio_path, "rb") as audio_file:
                response = requests.post(
                    url,
                    params=params,
                    headers=headers,
                    data=audio_file,
                    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶ï¼Œé˜²æ­¢è¶…å¤§æ–‡ä»¶æ–­è¿
                )
        except Exception as e:
            raise TranscriptionError(f"Deepgram è¯·æ±‚å¤±è´¥: {e}")
        if response.status_code != 200:
            raise TranscriptionError(f"Deepgram API æŠ¥é”™ ({response.status_code}): {response.text}")
        data = response.json()

        # 3. è§£æç»“æœ
        final_segments = []

        try:
            if "paragraphs" in data["results"]["channels"][0]["alternatives"][0]:
                paragraphs = data["results"]["channels"][0]["alternatives"][0]["paragraphs"]["paragraphs"]

                for p in paragraphs:
                    speaker_id = p.get("speaker", 0)
                    sentences = p["sentences"]
                    full_text = " ".join([s["text"] for s in sentences])
                    start_time = sentences[0]["start"]
                    end_time = sentences[-1]["end"]

                    final_segments.append({"start": start_time, "end": end_time, "text": full_text.strip(), "speaker": f"Speaker_{speaker_id}"})
            else:
                utterances = data["results"]["channels"][0]["alternatives"][0]["utterances"]
                for utt in utterances:
                    final_segments.append({"start": utt["start"], "end": utt["end"], "text": utt["transcript"].strip(), "speaker": f"Speaker_{utt.get('speaker', 0)}"})

        except KeyError:
            logger.warning("Deepgram è¿”å›äº†ç©ºç»“æœæˆ–æ ¼å¼å¼‚å¸¸ (å¯èƒ½æ˜¯é™éŸ³æ–‡ä»¶)")
            return []
        except Exception as e:
            raise TranscriptionError(f"è§£æ Deepgram ç»“æœå¤±è´¥: {e}")

        logger.success(f"[Deepgram] è½¬å½•å®Œæˆï¼Œå…± {len(final_segments)} ä¸ªæ®µè½")
        return final_segments
