import importlib.util
import os
from typing import Dict, List, Literal, Optional

import requests
from loguru import logger

HAS_WHISPERX = importlib.util.find_spec("whisperx") is not None
HAS_FUNASR = importlib.util.find_spec("funasr") is not None


# è¡¥ä¸å‡½æ•°
def _apply_torch_monkey_patch():
    import torch

    if getattr(torch, "_audigest_patched", False):
        return
    logger.debug("ğŸ”§ [Local] åº”ç”¨ PyTorch å…¼å®¹æ€§è¡¥ä¸...")
    _original_torch_load = torch.load

    def _safe_torch_load(*args, **kwargs):
        kwargs["weights_only"] = False
        return _original_torch_load(*args, **kwargs)

    torch.load = _safe_torch_load
    setattr(torch, "_audigest_patched", True)


class TranscriptionError(Exception):
    pass


class AudioTranscriber:
    def __init__(
        self,
        mode: Literal["local", "cloud"] = "local",
        api_key: Optional[str] = None,
        hf_token: Optional[str] = None,
        device: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–è½¬å½•å™¨
        :param mode: 'local' (WhisperX) æˆ– 'cloud' (Deepgram)
        :param api_key: äº‘ç«¯æ¨¡å¼çš„ API Key (Deepgram Key)
        :param hf_token: æœ¬åœ°æ¨¡å¼ Diarization å¿…é¡»çš„ HuggingFace Token
        """
        self.mode = mode
        self.api_key = api_key
        self.hf_token = hf_token
        self.device = device
        logger.info(f"[Transcriber] åˆå§‹åŒ–å®Œæˆ | æ¨¡å¼: {self.mode} | è®¾å¤‡: {self.device}")

    def transcribe(self, audio_path: str, language: str = "auto") -> List[Dict]:
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

        logger.info(f"[Transcriber] å¼€å§‹å¤„ç†: {audio_path} (Lang: {language})")

        try:
            if self.mode == "local":
                if language == "zh":
                    logger.info("ğŸ‡¨ğŸ‡³ æ£€æµ‹åˆ°ä¸­æ–‡ï¼Œåˆ‡æ¢è‡³ FunASR å¼•æ“...")
                    return self._transcribe_local_funasr(audio_path)
                else:
                    logger.info("ğŸŒ éä¸­æ–‡å†…å®¹ï¼Œä½¿ç”¨ WhisperX å¼•æ“...")
                    return self._transcribe_local_whisperx(audio_path)
            elif self.mode == "cloud":
                return self._transcribe_cloud_deepgram(audio_path, language=language)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {self.mode}")
        except Exception as e:
            logger.exception("âŒ [Transcriber] è½¬å½•å¤±è´¥")
            raise TranscriptionError(str(e)) from e

    def _transcribe_local_whisperx(self, audio_path: str) -> List[Dict]:
        if not HAS_WHISPERX:
            raise ImportError("æœªå®‰è£… whisperx æˆ– torchï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å¼ã€‚è¯·è¿è¡Œ uv add git+https://github.com/m-bain/whisperX.git")
        if not self.hf_token:
            logger.warning("âš ï¸ æœªæä¾› HuggingFace Tokenï¼Œæ— æ³•è¿›è¡Œè¯´è¯äººåˆ†ç¦» (Diarization)ï¼Œä»…èƒ½è½¬å½•æ–‡å­—ã€‚")
        _apply_torch_monkey_patch()
        import torch
        import whisperx
        from whisperx.diarize import DiarizationPipeline

        actual_device = self.device
        if actual_device is None:
            actual_device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ğŸ–¥ï¸ [Auto] è‡ªåŠ¨æ£€æµ‹åˆ°è¿è¡Œè®¾å¤‡: {actual_device}")

        model_name = "medium"
        compute_type = "int8" if actual_device == "cuda" else "int8"
        logger.info(f"â³ [Local] æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ ({model_name}, {compute_type})...")
        model = whisperx.load_model(model_name, actual_device, compute_type=compute_type)
        logger.info("[Local] æ­£åœ¨è½¬å½•æ–‡æœ¬...")
        result = model.transcribe(audio_path, batch_size=4)
        logger.info("[Local] æ­£åœ¨å¯¹é½æ—¶é—´è½´...")
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=actual_device)
        result = whisperx.align(result["segments"], model_a, metadata, audio_path, actual_device, return_char_alignments=False)
        if self.hf_token:
            logger.info("[Local] æ­£åœ¨è¯†åˆ«è¯´è¯äºº (Diarization)...")
            diarize_model = DiarizationPipeline(use_auth_token=self.hf_token, device=actual_device)
            diarize_segments = diarize_model(audio_path)

            result = whisperx.assign_word_speakers(diarize_segments, result)
        final_segments = []
        for segment in result["segments"]:
            final_segments.append(
                {
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip(),
                    "speaker": segment.get("speaker", "Unknown"),
                }
            )
        logger.success(f"âœ… [Local] è½¬å½•å®Œæˆï¼Œå…± {len(final_segments)} æ¡ç‰‡æ®µ")
        return final_segments

    def _transcribe_local_funasr(self, audio_path: str) -> List[Dict]:
        if not HAS_FUNASR:
            raise ImportError("æœªå®‰è£… funasrã€‚è¯·è¿è¡Œ uv add funasr modelscope")
        try:
            from funasr import AutoModel
        except ImportError:
            raise ImportError("FunASR å¯¼å…¥å¤±è´¥")
        logger.info("â³ [FunASR] æ­£åœ¨åŠ è½½æ¨¡å‹ (Paraformer-zh + Cam++)...")
        import torch

        actual_device = self.device
        if actual_device is None:
            actual_device = "cuda" if torch.cuda.is_available() else "cpu"

        model = AutoModel(
            model="paraformer-zh",
            model_revision="v2.0.4",
            vad_model="fsmn-vad",
            punc_model="ct-punc",
            spk_model="cam++",
            disable_update=True,
            device=actual_device,
        )

        logger.info("ğŸ—£ï¸ [FunASR] å¼€å§‹è½¬å½•...")

        try:
            res = model.generate(
                input=audio_path,
                batch_size_s=300,  # 300ç§’éŸ³é¢‘ä¸€æ‰¹ï¼Œæ˜¾å­˜ä¸å¤Ÿæ”¹å°
                return_spk_res=True,
            )
        except Exception as e:
            raise TranscriptionError(f"FunASR æ¨ç†é”™è¯¯: {e}")

        final_segments = []
        for item in res:
            if "sentence_info" in item:
                for sent in item["sentence_info"]:
                    spk_id = sent.get("spk", 0)

                    final_segments.append(
                        {
                            "start": sent["start"] / 1000.0,  # æ¯«ç§’ -> ç§’
                            "end": sent["end"] / 1000.0,
                            "text": sent["text"],
                            "speaker": f"Speaker_{spk_id}",
                        }
                    )
        logger.success(f"âœ… [FunASR] ä¸­æ–‡è½¬å½•å®Œæˆï¼Œå…± {len(final_segments)} æ¡")
        return final_segments

    def _transcribe_cloud_deepgram(self, audio_path: str, language: str = "auto") -> List[Dict]:
        if not self.api_key:
            raise ValueError("ä½¿ç”¨ Deepgram æ¨¡å¼å¿…é¡»æä¾› api_key")

        url = "https://api.deepgram.com/v1/listen"
        params = {
            "model": "nova-2",
            "smart_format": "true",
            "diarize": "true",  # å¼€å¯è¯´è¯äººåˆ†ç¦»
            "punctuate": "true",
            "utterances": "true",
        }
        if language and language != "auto":
            params["language"] = language
        else:
            params["detect_language"] = "true"

        headers = {
            "Authorization": f"Token {self.api_key}",
            "Content-Type": "audio/*",
        }

        logger.info(f"[Deepgram] å¼€å§‹ä¸Šä¼ å¹¶è½¬å½• (è¯­è¨€: {language})...")

        try:
            with open(audio_path, "rb") as audio_file:
                response = requests.post(
                    url,
                    params=params,
                    headers=headers,
                    data=audio_file,
                    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶ï¼Œé˜²æ­¢è¶…å¤§æ–‡ä»¶æ–­è¿
                )
        except Exception as e:
            raise TranscriptionError(f"Deepgram è¯·æ±‚å¤±è´¥: {e}")
        if response.status_code != 200:
            raise TranscriptionError(f"Deepgram API æŠ¥é”™ ({response.status_code}): {response.text}")
        data = response.json()

        final_segments = []

        try:
            if "paragraphs" in data["results"]["channels"][0]["alternatives"][0]:
                paragraphs = data["results"]["channels"][0]["alternatives"][0]["paragraphs"]["paragraphs"]

                for p in paragraphs:
                    speaker_id = p.get("speaker", 0)
                    sentences = p["sentences"]
                    full_text = " ".join([s["text"] for s in sentences])
                    start_time = sentences[0]["start"]
                    end_time = sentences[-1]["end"]

                    final_segments.append({"start": start_time, "end": end_time, "text": full_text.strip(), "speaker": f"Speaker_{speaker_id}"})
            else:
                utterances = data["results"]["channels"][0]["alternatives"][0]["utterances"]
                for utt in utterances:
                    final_segments.append({"start": utt["start"], "end": utt["end"], "text": utt["transcript"].strip(), "speaker": f"Speaker_{utt.get('speaker', 0)}"})

        except KeyError:
            logger.warning("Deepgram è¿”å›äº†ç©ºç»“æœæˆ–æ ¼å¼å¼‚å¸¸ (å¯èƒ½æ˜¯é™éŸ³æ–‡ä»¶)")
            return []
        except Exception as e:
            raise TranscriptionError(f"è§£æ Deepgram ç»“æœå¤±è´¥: {e}")

        logger.success(f"[Deepgram] è½¬å½•å®Œæˆï¼Œå…± {len(final_segments)} ä¸ªæ®µè½")
        return final_segments
